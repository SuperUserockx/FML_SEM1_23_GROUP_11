{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6VjjZEavq6M",
        "outputId": "ba32fb9b-c4f5-4d92-8e71-017f5bb5cba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.1.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1.post0\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.1.1.post0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373075 sha256=83ea4095cd5874157e6d2ff9520bcc99c9ccd59793d664f723464e67939dae8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8aVSECtT7Vmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f47b273d-13dd-4b5c-8fb3-aa464c96bdd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos/BipedalWalker-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment BipedalWalker-v3 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment BipedalWalker-v3 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment BipedalWalker-v3 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:288: UserWarning: \u001b[33mWARN: Tried to pass invalid video frame, marking as broken: Your frame has shape (400, 600, 3), but the VideoRecorder is configured for shape (400, 521, 3).\u001b[0m\n",
            "  logger.warn(\"Tried to pass invalid video frame, marking as broken: %s\", e)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment BipedalWalker-v3 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved weights for 0th iteration\n",
            "Step:  0 Reward:  -0.6097892389101282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment BipedalWalker-v3 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step:  1 Reward:  4.629004104980046\n",
            "Saved weights for 1th iteration\n",
            "Step:  2 Reward:  1.4183342760986832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment BipedalWalker-v3 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step:  3 Reward:  -1.6302967857707102\n",
            "Step:  4 Reward:  1.68351240220969\n",
            "Step:  5 Reward:  0.7412366998581716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment BipedalWalker-v3 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step:  6 Reward:  0.9104387705325088\n",
            "Step:  7 Reward:  1.1686486853335238\n",
            "Step:  8 Reward:  0.17544746225786456\n",
            "Step:  9 Reward:  1.6114862841897017\n"
          ]
        }
      ],
      "source": [
        "import os #used for interacting with the operating system\n",
        "import numpy as np #to make array related operations\n",
        "import gym #to get the enviroment\n",
        "from gym import wrappers #to record videos of the enviroment\n",
        "# import pybullet_envs\n",
        "max_reward = 0\n",
        "\n",
        "class Normalizer():\n",
        "    # Normalizes the inputs\n",
        "    def __init__(self, nb_inputs):\n",
        "        self.n = np.zeros(nb_inputs) #for number of obseravations for each input dimension\n",
        "        self.mean = np.zeros(nb_inputs) #for running mean\n",
        "        self.mean_diff = np.zeros(nb_inputs) #to update the running mean\n",
        "        self.var = np.zeros(nb_inputs) #for running variance\n",
        "\n",
        "\n",
        "    def observe(self, x): #here x is observation made\n",
        "        self.n += 1.0 #incrementing the number of observation\n",
        "        last_mean = self.mean.copy() #making last mean equal to the running mean\n",
        "        self.mean += (x - self.mean) / self.n #mean is getting updated to = last_mean + (observation - last_mean)/number_of_observations\n",
        "        self.mean_diff += (x - last_mean) * (x - self.mean) #getting mean difference\n",
        "        self.var = (self.mean_diff / self.n).clip(min = 1e-2) #calculating the variance where clip avoids getting divided by 0\n",
        "\n",
        "    def normalize(self, inputs):\n",
        "        obs_mean = self.mean #getting mean\n",
        "        obs_std = np.sqrt(self.var) #getting the standard deviation\n",
        "        return (inputs - obs_mean) / obs_std #returning the normalized value\n",
        "\n",
        "## Algorithm\n",
        "class Walker(): #giving hyperparameters\n",
        "    def __init__(self,nb_steps=10, episode_length=2000, learning_rate=0.05, num_deltas=16, num_best_deltas=16, noise=0.04, seed=1, env_name='BipedalWalker-v3',record_every=25, monitor_dir = None):\n",
        "        self.nb_steps = nb_steps #number of training steps\n",
        "        self.episode_length = episode_length #maximum number of steps in each episode\n",
        "        self.learning_rate = learning_rate #giving the learning rate that is the alpha\n",
        "        self.num_deltas = num_deltas #number of noise that is delta\n",
        "        self.num_best_deltas = num_best_deltas #number of top deltas that we want to store\n",
        "        assert self.num_best_deltas <= self.num_deltas\n",
        "        self.noise = noise #value of noise\n",
        "        self.seed = seed #gives the same random each time\n",
        "        self.record_every = record_every #recording video after every 25 episode\n",
        "        np.random.seed(self.seed) #producing same random number each time at the start of the experiment to compare the results\n",
        "        self.env = gym.make(env_name) #making the enviroment\n",
        "        if monitor_dir is not None:\n",
        "            should_record = lambda i: self.record_video\n",
        "            self.env = wrappers.RecordVideo(self.env, monitor_dir) #recording video of enviorement and storing in the given directory\n",
        "        self.input_size = self.env.observation_space.shape[0] #number of neurons at input layer\n",
        "        self.output_size = self.env.action_space.shape[0] #number of neurons at output layer\n",
        "        self.normalizer = Normalizer(self.input_size) #inheriting the normalizer class\n",
        "        self.episode_length = self.env.spec.max_episode_steps or episode_length\n",
        "        self.theta = np.zeros((self.output_size, self.input_size)) #fiving initial weights that is theta\n",
        "        self.record_video = False #signifies that the video recording during theb training is turned of\n",
        "\n",
        "    def sample_deltas(self):\n",
        "        return [np.random.randn(*self.theta.shape) for _ in range(self.num_deltas)] #initialising noises\n",
        "\n",
        "    def evaluate(self, input, delta = None, direction = None): #getting output by dot product of weights and inputs\n",
        "        if direction is None:\n",
        "            return self.theta.dot(input)\n",
        "        elif direction == \"+\":\n",
        "            return (self.theta + self.noise * delta).dot(input)\n",
        "        elif direction == \"-\":\n",
        "            return (self.theta - self.noise * delta).dot(input)\n",
        "\n",
        "    def play_episode(self, direction=None, delta=None, render=False): #things that will occur during an episode\n",
        "        state = self.env.reset() #reseting the enviroment to initial state\n",
        "        done = False #indicating that the episode is not ended yet\n",
        "        num_plays = 0.0\n",
        "        sum_rewards = 0.0\n",
        "        while not done and num_plays < self.episode_length: #while episode is not ended and number of steps are less than the max number of steps that could be in an episode\n",
        "            self.normalizer.observe(state)\n",
        "            state = self.normalizer.normalize(state) #getting the z score\n",
        "            action = self.evaluate(state,delta,direction) #getting output as actions\n",
        "            state, reward, done, _ = self.env.step(action) #storing the hp for new state in state variable, reward obtained in the step in the reward variable after taking the action\n",
        "            reward = max(min(reward, 1), -1)\n",
        "            sum_rewards += reward #summing the reward obtained in this step to the reward obtained in in the preivous steps\n",
        "            num_plays += 1 #increase the time step by 1 as the previous step is completed\n",
        "            if render:\n",
        "                self.env.render()\n",
        "\n",
        "        return sum_rewards #reward at the end of the episode\n",
        "\n",
        "    def train(self):\n",
        "        for iteration in range(self.nb_steps):\n",
        "            # Generate num_deltas deltas and evaluate positive and negative rewards\n",
        "            deltas = self.sample_deltas()\n",
        "            positive_rewards = [0] * self.num_deltas #initialising matrix with 0 to store the rewards\n",
        "            negative_rewards = [0] * self.num_deltas #initialising matrix with 0 to store the rewards\n",
        "\n",
        "            # Run num_deltas episode with positive and negative variations\n",
        "            for i in range(self.num_deltas):\n",
        "                positive_rewards[i] = self.play_episode(direction=\"+\",delta=deltas[i])\n",
        "                negative_rewards[i] = self.play_episode(direction=\"-\",delta=deltas[i])\n",
        "\n",
        "            # Collect rollouts r+,r-,delta\n",
        "            #rollouts = zip(positive_rewards, negative_rewards, deltas)\n",
        "\n",
        "            # Calculate the standard deviation of all the rewards\n",
        "            sigma_rewards = np.array(positive_rewards + negative_rewards).std()\n",
        "\n",
        "            # Sort the rollouts by maximum reward and select best_num_deltas rollouts\n",
        "            scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards,negative_rewards))} #making the dictionary with index k and storing the max outof +r and -r\n",
        "            order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:self.num_best_deltas] #sorting in descending order and selects the top num_best_deltas values\n",
        "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order] #creating rollouts tuple for the best performing deltas\n",
        "\n",
        "            # Calculate step\n",
        "            step = np.zeros(self.theta.shape)\n",
        "            for pos, neg, d in rollouts:\n",
        "                step += (pos-neg)*d\n",
        "\n",
        "            # Update the weights\n",
        "            self.theta += self.learning_rate/(self.num_best_deltas*sigma_rewards) * step #updating the weights\n",
        "\n",
        "            # Only record video during evaluation, every n steps\n",
        "            if iteration % self.record_every == 0:\n",
        "                self.record_video = True\n",
        "                np.save(os.path.join(\"weights\", \"weights_\" + str(iteration)), self.theta)\n",
        "                print(\"Saved weights for \" + str(iteration) + \"th iteration\")\n",
        "\n",
        "            # Play an episode with the new weights and see improvement\n",
        "            final_reward = self.play_episode() ## We play without + or - noise\n",
        "            print('Step: ', iteration, 'Reward: ', final_reward)\n",
        "            with open(os.path.join(\"data\",\"data.txt\"), \"a\") as my_file:\n",
        "                my_file.write(str(iteration) + \", \" + str(final_reward) + \"\\n\")\n",
        "\n",
        "            global max_reward\n",
        "            if final_reward > max_reward:\n",
        "                max_reward = final_reward\n",
        "                np.save(os.path.join(\"max_weights\",\"weights_\" + str(iteration)), self.theta)\n",
        "                print(\"Saved weights for \" + str(iteration) + \"th iteration\")\n",
        "                self.record_video = True\n",
        "                with open(os.path.join(\"data\",\"max.txt\"), \"a\") as my_file:\n",
        "                    my_file.write(str(iteration) + \", \" + str(final_reward) + \"\\n\")\n",
        "            final_reward = self.play_episode()\n",
        "\n",
        "            self.record_video = False\n",
        "\n",
        "\n",
        "def mkdir(base, name): #creating paths to store data\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "\n",
        "# Main code\n",
        "if __name__ == '__main__':\n",
        "    ENV_NAME = \"BipedalWalker-v3\"\n",
        "    videos_dir = mkdir('.', 'videos')\n",
        "    monitor_dir = mkdir(videos_dir, ENV_NAME)\n",
        "    mkdir('.', 'weights')\n",
        "    mkdir('.', 'max_weights')\n",
        "    mkdir('.', 'data')\n",
        "    trainer = Walker(seed = 1000,monitor_dir=monitor_dir)\n",
        "    trainer.train()\n",
        "#     # ...\n",
        "\n"
      ]
    }
  ]
}