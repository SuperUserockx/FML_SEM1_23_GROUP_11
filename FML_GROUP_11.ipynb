{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6VjjZEavq6M",
        "outputId": "ba32fb9b-c4f5-4d92-8e71-017f5bb5cba9"
      },
      "outputs": [],
      "source": [
        "!pip install swig\n",
        "!pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aVSECtT7Vmj",
        "outputId": "f47b273d-13dd-4b5c-8fb3-aa464c96bdd6"
      },
      "outputs": [],
      "source": [
        "import os #used for interacting with the operating system\n",
        "import numpy as np #to make array related operations\n",
        "import gym #to get the enviroment\n",
        "from gym import wrappers #to record videos of the enviroment\n",
        "# import pybullet_envs\n",
        "max_reward = 0\n",
        "\n",
        "class Normalizer():\n",
        "    # Normalizes the inputs\n",
        "    def __init__(self, nb_inputs):\n",
        "        self.n = np.zeros(nb_inputs) #for number of obseravations for each input dimension\n",
        "        self.mean = np.zeros(nb_inputs) #for running mean\n",
        "        self.mean_diff = np.zeros(nb_inputs) #to update the running mean\n",
        "        self.var = np.zeros(nb_inputs) #for running variance\n",
        "\n",
        "\n",
        "    def observe(self, x): #here x is observation made\n",
        "        self.n += 1.0 #incrementing the number of observation\n",
        "        last_mean = self.mean.copy() #making last mean equal to the running mean\n",
        "        self.mean += (x - self.mean) / self.n #mean is getting updated to = last_mean + (observation - last_mean)/number_of_observations\n",
        "        self.mean_diff += (x - last_mean) * (x - self.mean) #getting mean difference\n",
        "        self.var = (self.mean_diff / self.n).clip(min = 1e-2) #calculating the variance where clip avoids getting divided by 0\n",
        "\n",
        "    def normalize(self, inputs):\n",
        "        obs_mean = self.mean #getting mean\n",
        "        obs_std = np.sqrt(self.var) #getting the standard deviation\n",
        "        return (inputs - obs_mean) / obs_std #returning the normalized value\n",
        "\n",
        "## Algorithm\n",
        "class Walker(): #giving hyperparameters\n",
        "    def __init__(self,nb_steps=10000, episode_length=2000, learning_rate=0.05, num_deltas=16, num_best_deltas=16, noise=0.04, seed=1, env_name='BipedalWalker-v3',record_every=25, monitor_dir = None):\n",
        "        self.nb_steps = nb_steps #number of training steps\n",
        "        self.episode_length = episode_length #maximum number of steps in each episode\n",
        "        self.learning_rate = learning_rate #giving the learning rate that is the alpha\n",
        "        self.num_deltas = num_deltas #number of noise that is delta\n",
        "        self.num_best_deltas = num_best_deltas #number of top deltas that we want to store\n",
        "        assert self.num_best_deltas <= self.num_deltas\n",
        "        self.noise = noise #value of noise\n",
        "        self.seed = seed #gives the same random each time\n",
        "        self.record_every = record_every #recording video after every 25 episode\n",
        "        np.random.seed(self.seed) #producing same random number each time at the start of the experiment to compare the results\n",
        "        self.env = gym.make(env_name) #making the enviroment\n",
        "        if monitor_dir is not None:\n",
        "            should_record = lambda i: self.record_video\n",
        "            self.env = wrappers.RecordVideo(self.env, monitor_dir) #recording video of enviorement and storing in the given directory\n",
        "        self.input_size = self.env.observation_space.shape[0] #number of neurons at input layer\n",
        "        self.output_size = self.env.action_space.shape[0] #number of neurons at output layer\n",
        "        self.normalizer = Normalizer(self.input_size) #inheriting the normalizer class\n",
        "        self.episode_length = self.env.spec.max_episode_steps or episode_length\n",
        "        self.theta = np.zeros((self.output_size, self.input_size)) #fiving initial weights that is theta\n",
        "        self.record_video = False #signifies that the video recording during theb training is turned of\n",
        "\n",
        "    def sample_deltas(self):\n",
        "        return [np.random.randn(*self.theta.shape) for _ in range(self.num_deltas)] #initialising noises\n",
        "\n",
        "    def evaluate(self, input, delta = None, direction = None): #getting output by dot product of weights and inputs\n",
        "        if direction is None:\n",
        "            return self.theta.dot(input)\n",
        "        elif direction == \"+\":\n",
        "            return (self.theta + self.noise * delta).dot(input)\n",
        "        elif direction == \"-\":\n",
        "            return (self.theta - self.noise * delta).dot(input)\n",
        "\n",
        "    def play_episode(self, direction=None, delta=None, render=False): #things that will occur during an episode\n",
        "        state = self.env.reset() #reseting the enviroment to initial state\n",
        "        done = False #indicating that the episode is not ended yet\n",
        "        num_plays = 0.0\n",
        "        sum_rewards = 0.0\n",
        "        while not done and num_plays < self.episode_length: #while episode is not ended and number of steps are less than the max number of steps that could be in an episode\n",
        "            self.normalizer.observe(state)\n",
        "            state = self.normalizer.normalize(state) #getting the z score\n",
        "            action = self.evaluate(state,delta,direction) #getting output as actions\n",
        "            state, reward, done, _ = self.env.step(action) #storing the hp for new state in state variable, reward obtained in the step in the reward variable after taking the action\n",
        "            reward = max(min(reward, 1), -1)\n",
        "            sum_rewards += reward #summing the reward obtained in this step to the reward obtained in in the preivous steps\n",
        "            num_plays += 1 #increase the time step by 1 as the previous step is completed\n",
        "            if render:\n",
        "                self.env.render()\n",
        "\n",
        "        return sum_rewards #reward at the end of the episode\n",
        "\n",
        "    def train(self):\n",
        "        for iteration in range(self.nb_steps):\n",
        "            # Generate num_deltas deltas and evaluate positive and negative rewards\n",
        "            deltas = self.sample_deltas()\n",
        "            positive_rewards = [0] * self.num_deltas #initialising matrix with 0 to store the rewards\n",
        "            negative_rewards = [0] * self.num_deltas #initialising matrix with 0 to store the rewards\n",
        "\n",
        "            # Run num_deltas episode with positive and negative variations\n",
        "            for i in range(self.num_deltas):\n",
        "                positive_rewards[i] = self.play_episode(direction=\"+\",delta=deltas[i])\n",
        "                negative_rewards[i] = self.play_episode(direction=\"-\",delta=deltas[i])\n",
        "\n",
        "            # Collect rollouts r+,r-,delta\n",
        "            #rollouts = zip(positive_rewards, negative_rewards, deltas)\n",
        "\n",
        "            # Calculate the standard deviation of all the rewards\n",
        "            sigma_rewards = np.array(positive_rewards + negative_rewards).std()\n",
        "\n",
        "            # Sort the rollouts by maximum reward and select best_num_deltas rollouts\n",
        "            scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards,negative_rewards))} #making the dictionary with index k and storing the max outof +r and -r\n",
        "            order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:self.num_best_deltas] #sorting in descending order and selects the top num_best_deltas values\n",
        "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order] #creating rollouts tuple for the best performing deltas\n",
        "\n",
        "            # Calculate step\n",
        "            step = np.zeros(self.theta.shape)\n",
        "            for pos, neg, d in rollouts:\n",
        "                step += (pos-neg)*d\n",
        "\n",
        "            # Update the weights\n",
        "            self.theta += self.learning_rate/(self.num_best_deltas*sigma_rewards) * step #updating the weights\n",
        "\n",
        "            # Only record video during evaluation, every n steps\n",
        "            if iteration % self.record_every == 0:\n",
        "                self.record_video = True\n",
        "                np.save(os.path.join(\"weights\", \"weights_\" + str(iteration)), self.theta)\n",
        "                print(\"Saved weights for \" + str(iteration) + \"th iteration\")\n",
        "\n",
        "            # Play an episode with the new weights and see improvement\n",
        "            final_reward = self.play_episode() ## We play without + or - noise\n",
        "            print('Step: ', iteration, 'Reward: ', final_reward)\n",
        "            with open(os.path.join(\"data\",\"data.txt\"), \"a\") as my_file:\n",
        "                my_file.write(str(iteration) + \", \" + str(final_reward) + \"\\n\")\n",
        "\n",
        "            global max_reward\n",
        "            if final_reward > max_reward:\n",
        "                max_reward = final_reward\n",
        "                np.save(os.path.join(\"max_weights\",\"weights_\" + str(iteration)), self.theta)\n",
        "                print(\"Saved weights for \" + str(iteration) + \"th iteration\")\n",
        "                self.record_video = True\n",
        "                with open(os.path.join(\"data\",\"max.txt\"), \"a\") as my_file:\n",
        "                    my_file.write(str(iteration) + \", \" + str(final_reward) + \"\\n\")\n",
        "            final_reward = self.play_episode()\n",
        "\n",
        "            self.record_video = False\n",
        "\n",
        "\n",
        "def mkdir(base, name): #creating paths to store data\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "\n",
        "# Main code\n",
        "if __name__ == '__main__':\n",
        "    ENV_NAME = \"BipedalWalker-v3\"\n",
        "    videos_dir = mkdir('.', 'videos')\n",
        "    monitor_dir = mkdir(videos_dir, ENV_NAME)\n",
        "    mkdir('.', 'weights')\n",
        "    mkdir('.', 'max_weights')\n",
        "    mkdir('.', 'data')\n",
        "    trainer = Walker(seed = 1000,monitor_dir=monitor_dir)\n",
        "    trainer.train()\n",
        "#     # ...\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
